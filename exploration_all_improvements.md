# Sturgeon â€” Comprehensive Improvement Exploration

Full audit of every actionable improvement across code quality, features, system design, and stretch goals. Grouped by area, tagged by effort/impact.

---

## 1. Frontend â€” UX & Features

### 1A. Input Validation on Upload Page

**File**: [page.tsx](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/frontend/app/page.tsx#L155)
**Problem**: `handleAnalyze` silently returns when no input is provided â€” no toast, no shake, no red border. User clicks "Analyze" and nothing happens.
**Fix**: Add validation state, show an error message below the button or a toast when neither patient history nor any evidence (image/lab file) is present.
**Effort**: ğŸŸ¢ Small (~30 min) | **Impact**: ğŸ”´ High (usability blocker)

---

### 1B. Suggested Challenge Prompts in Debate UI

**File**: [debate/page.tsx](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/frontend/app/debate/page.tsx)
**Problem**: Users (especially non-clinicians) don't know _what_ to type to challenge the AI. The text field is blank with no guidance.
**Fix**: Add 3-4 contextual chip/pill buttons above the input field (e.g., "What if CRP is elevated?", "Could this be autoimmune?", "What test would help differentiate?"). Clicking sends that text as the challenge. Chips could be:

- Static defaults (always available)
- Dynamic (generated by Gemini in the `suggested_test` response, or from a new field `suggested_challenges`)
  **Considerations**:
- Static is simpler; dynamic requires a backend change to return `suggested_challenges[]` in `DebateTurnResponse`
- Could also pull from the current differential's `suggested_tests` field
  **Effort**: ğŸŸ¡ Medium (1-2h static, 3-4h dynamic) | **Impact**: ğŸ”´ High (engagement)

---

### 1C. Export Case as PDF

**Files**: [summary/page.tsx](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/frontend/app/summary/page.tsx), (new utility)
**Problem**: After a full debate + summary, there's no way to save/export the case. Everything is ephemeral (localStorage only).
**Fix options**:

1. **Browser `window.print()`** â€” simplest, uses CSS `@media print` to hide nav and style for paper. Zero dependencies.
2. **`html2canvas` + `jspdf`** â€” generates a screenshot-based PDF. More control over layout but adds 2 npm deps (~100KB gzipped).
3. **`@react-pdf/renderer`** â€” pure React-to-PDF renderer. Most control but heaviest dependency and most code.
4. **Backend-generated PDF** â€” Python `reportlab` or `weasyprint`. Adds a new `/export-pdf` endpoint.
   **Recommendation**: Start with `window.print()` + print CSS (0 deps). Upgrade to `jspdf` later if needed.
   **PDF should include**: Patient history, lab values, differential evolution, debate rounds, final diagnosis, reasoning chain, next steps.
   **Effort**: ğŸŸ¢ Small (print CSS) to ğŸ”´ Large (full PDF renderer) | **Impact**: ğŸŸ¡ Medium

---

### 1D. Loading State Improvements (Debate Page)

**File**: [debate/page.tsx](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/frontend/app/debate/page.tsx#L350-L370)
**Current**: Dot-pulse animation with "Gemini + MedGemma are reasoning..." or "MedGemma is thinking..." text.
**Improvements**:

- **Multi-phase loading text**: Cycle through phases:
  1. "Gemini is formulating a clinical question..." (0-3s)
  2. "MedGemma is analyzing the evidence..." (3-8s)
  3. "Synthesizing response..." (8s+)
- **HeroUI Skeleton**: Replace the dot-pulse with HeroUI `<Skeleton>` components mimicking a chat bubble shape (already imported from `@heroui/react`)
- **Elapsed timer**: Show "Processing... (12s)" to set expectations
  **Effort**: ğŸŸ¢ Small (phase text) to ğŸŸ¡ Medium (skeleton) | **Impact**: ğŸŸ¡ Medium

---

### 1E. Dark Mode Toggle

**File**: [globals.css](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/frontend/app/globals.css)
**Current**: Light-only theme with CSS custom properties.
**Fix**: Add a `:root[data-theme="dark"]` block with dark equivalents. Add a toggle button in the header. Store preference in localStorage.
**Effort**: ğŸŸ¡ Medium (2-3h) | **Impact**: ğŸŸ¢ Low (nice-to-have, demos often use dark mode)

---

### 1F. Responsive / Mobile Layout

**Current**: Desktop-optimized layout. Debate page sidebar + chat doesn't collapse gracefully on mobile.
**Fix**: Add responsive breakpoint handling. Sidebar becomes a collapsible drawer on mobile. Chat takes full width.
**Effort**: ğŸŸ¡ Medium (3-4h) | **Impact**: ğŸŸ¡ Medium (accessibility prize consideration)

---

### 1G. Accessibility Improvements

**Current**: No ARIA labels on interactive elements. Images lack alt text. Color contrast may not meet WCAG AA.
**Fix**:

- Add `aria-label` to all buttons and interactive elements
- Add `aria-live="polite"` to loading indicators for screen readers
- Ensure color contrast ratios meet WCAG AA (4.5:1 for normal text)
- Add keyboard navigation support for debate chips
  **Effort**: ğŸŸ¡ Medium (2-3h) | **Impact**: ğŸŸ¡ Medium (inclusivity, judges may notice)

---

### 1H. Debate History Sidebar Enhancement

**Current**: Sidebar shows static differential. Chat shows linear history.
**Improvements**:

- **Probability bar charts**: Visual bars showing probability shift over rounds
- **Diff highlighting**: When a diagnosis probability changes, flash the change (green for up, red for down)
- **Collapsible evidence sections**: Supporting/against evidence in expandable accordions
  **Effort**: ğŸŸ¡ Medium (3-4h) | **Impact**: ğŸŸ¡ Medium (demo wow factor)

---

## 2. Backend â€” Code Quality & Architecture

### 2A. Split `main.py` (940 lines) into Modules

**File**: [main.py](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/ai-service/main.py)
**Problem**: Single monolithic file with models, utilities, and all endpoints.
**Proposed split**:

- `models.py` â€” All Pydantic request/response models (lines 116-184)
- `json_utils.py` â€” `extract_json`, `_repair_truncated_json`, `_fix_newlines_in_json_strings` (lines 186-344)
- `refusal.py` â€” `_is_pure_refusal`, `_strip_refusal_preamble` (lines 347-421)
- `formatters.py` â€” `format_lab_values`, `format_differential`, `format_rounds` (lines 424-458)
- `main.py` â€” Slim: just app setup, lifespan, and endpoint definitions (importing from above)
  **Effort**: ğŸŸ¢ Small (1-2h, mechanical) | **Impact**: ğŸŸ¡ Medium (maintainability, code review)

---

### 2B. Backend Unit Tests

**Current**: Zero tests exist in the project.
**Targets** (pure functions, no model needed):

- `_repair_truncated_json()` â€” edge cases with nested arrays, strings, truncation points
- `_fix_newlines_in_json_strings()` â€” newlines inside strings, escaped quotes
- `extract_json()` â€” code blocks, truncated output, regex fallback
- `_is_pure_refusal()` â€” pure refusal detection, real analysis with disclaimers
- `_strip_refusal_preamble()` â€” preamble stripping with various transition phrases
- `format_lab_values/differential/rounds()` â€” formatting edge cases
- `_parse_differential()` â€” field name variations from MedGemma/Gemini
  **Framework**: `pytest` (add to `requirements.txt`)
  **Effort**: ğŸŸ¡ Medium (3-4h) | **Impact**: ğŸŸ¡ Medium (confidence in JSON utilities)

---

### 2C. Error Handling & Graceful Degradation

**Current gaps**:

- `generate_differential` endpoint has no try/except â€” raw HTTP 500 on MedGemma failure
- `generate_summary` endpoint has no try/except â€” same issue
- `extract_labs` endpoint has no try/except â€” same issue
- When MedGemma times out or VRAM OOMs, user gets a generic "Failed to connect to AI service"
  **Fix**:
- Wrap all endpoints in try/except with meaningful error messages
- Add timeout handling (configurable `INFERENCE_TIMEOUT` env var)
- Return partial results when possible (e.g., "Generated 2 of 3 diagnoses before timeout")
- Frontend: show error toasts instead of silent failures
  **Effort**: ğŸŸ¡ Medium (2-3h) | **Impact**: ğŸ”´ High (robustness)

---

### 2D. Request Validation

**Current**: Backend accepts empty strings for `patient_history`, empty dicts for `lab_values`.
**Fix**: Add Pydantic validators:

- `patient_history` must be non-empty (at least 10 chars?)
- `DifferentialRequest` should require either non-empty `patient_history` OR non-empty `lab_values`
- `DebateTurnRequest.user_challenge` must be non-empty
  **Effort**: ğŸŸ¢ Small (30 min) | **Impact**: ğŸŸ¡ Medium

---

### 2E. Streaming Responses (SSE)

**Current**: All endpoints return complete responses. Debate turns take 8-15s with no intermediate feedback.
**Fix**: Use FastAPI `StreamingResponse` with Server-Sent Events:

```
event: phase
data: {"phase": "querying_medgemma", "elapsed": 3.2}

event: phase
data: {"phase": "synthesizing", "elapsed": 8.1}

event: result
data: {"ai_response": "...", "updated_differential": [...]}
```

**Frontend**: Use `EventSource` or `fetch` + `ReadableStream` to consume.
**Considerations**: This fundamentally changes the API contract. Both API routes and frontend need updates.
**Effort**: ğŸ”´ Large (4-6h) | **Impact**: ğŸŸ¡ Medium (perceived performance)

---

### 2F. Session Cleanup / TTL

**Current**: `_sessions` dict grows indefinitely â€” no cleanup.
**Fix**: Add a TTL (e.g., 1 hour) and periodic cleanup. Or use `max_sessions` cap with LRU eviction.
**Effort**: ğŸŸ¢ Small (30 min) | **Impact**: ğŸŸ¢ Low (only matters under load)

---

### 2G. CORS Hardening

**File**: [main.py L107-113](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/ai-service/main.py#L107-L113)
**Current**: Only allows `http://localhost:3000`.
**Fix**: Make configurable via `ALLOWED_ORIGINS` env var. Add Vercel domain when deploying.
**Effort**: ğŸŸ¢ Small (10 min) | **Impact**: ğŸŸ¡ Medium (needed for Vercel deploy)

---

## 3. Prompt Engineering

### 3A. Few-Shot Examples in Prompts

**Files**: [prompts.py](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/ai-service/prompts.py)
**Current**: All prompts are zero-shot with template examples. MedGemma sometimes returns inconsistent field names or formats.
**Fix**: Add 1-2 complete input/output examples to each prompt. This significantly improves JSON compliance.
**Trade-off**: Increases prompt length â†’ more input tokens â†’ slightly slower inference.
**Effort**: ğŸŸ¡ Medium (1-2h) | **Impact**: ğŸŸ¡ Medium (consistency)

---

### 3B. Chain-of-Thought for Differential

**Current**: `DIFFERENTIAL_PROMPT` asks for diagnoses directly.
**Fix**: Add "Think step by step" instruction before the JSON output requirement. MedGemma 1.5 responds well to CoT prompting.
**Format**: Ask for reasoning first (could be extracted into `reasoning_chain` field), then structured output.
**Effort**: ğŸŸ¢ Small (30 min) | **Impact**: ğŸŸ¡ Medium (analysis quality)

---

### 3C. Confidence Calibration Prompt

**Current**: `SUMMARY_PROMPT` asks for "high/medium/low" confidence â€” very coarse.
**Fix**: Add a numeric confidence scale (0-100) with calibration instructions:

> "Rate your confidence as a percentage. 90+ means you would bet on this diagnosis. 50-70 means you have significant uncertainty. Below 50 means you're unsure."
> **Effort**: ğŸŸ¢ Small (20 min) | **Impact**: ğŸŸ¢ Low (more granular for UI)

---

### 3D. Image Context in Debate Prompts

**Current**: MedGemma-only debate turns (`DEBATE_TURN_PROMPT`) don't include image analysis context.
**Fixed in orchestrated mode** (via `ClinicalState.image_context`), but the fallback path ignores it.
**Fix**: Add `{image_context}` section to `DEBATE_TURN_PROMPT` when available.
**Effort**: ğŸŸ¢ Small (20 min) | **Impact**: ğŸŸ¡ Medium (completeness)

---

## 4. Stretch Goals â€” System Design

### 4A. RAG with Clinical Guidelines ğŸ†

**From**: [STURGEON_PROJECT_PLAN.md L512](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/STURGEON_PROJECT_PLAN.md#L512)
**Concept**: Enhance MedGemma's reasoning by retrieving relevant clinical guidelines during the debate.
**Implementation approaches**:

#### Option A: Gemini Grounding (Simplest)

- Use Gemini's built-in grounding with Google Search or custom data
- Gemini already acts as orchestrator â€” add a grounding step before/after MedGemma
- Feed retrieved guidelines into MedGemma's context alongside clinical data
- **Pros**: No vector DB, no embeddings, minimal code change
- **Cons**: Limited to what Google indexes, less control over sources

#### Option B: Local Embeddings + Vector Search

- Use `sentence-transformers` or MedCPT for medical text embeddings
- Store clinical guidelines (UpToDate summaries, WHO guidelines, diagnostic criteria) in ChromaDB or FAISS
- On each debate turn, embed the current differential + challenge â†’ retrieve top-k relevant guidelines
- Feed retrieved text into Gemini/MedGemma prompt context
- **Architecture**:
  ```
  User challenge â†’ Embed query â†’ Vector search â†’ Top-k guidelines
                                                        â†“
  Gemini prompt = clinical_state + challenge + retrieved_guidelines
  ```
- **Guideline sources** (open access):
  - WHO clinical guidelines (public domain)
  - OpenMRS clinical decision support rules
  - WikiDoc differential diagnosis pages
  - PubMed abstracts (via Entrez API)
- **Deps**: `chromadb`, `sentence-transformers` (~500MB model)
- **Pros**: Full control, works offline, reproducible
- **Cons**: Needs guideline corpus curation, embedding model adds memory

#### Option C: MedGemma's Built-in Knowledge + Structured Retrieval

- MedGemma 1.5 already knows clinical guidelines â€” it was trained on medical literature
- Instead of external RAG, use a "knowledge elicitation" prompt pattern:
  > "Before answering, first recall the relevant diagnostic criteria for [condition] from clinical guidelines (e.g., DSM-5, ADA guidelines, NICE guidelines). Then apply those criteria to this case."
- **Pros**: Zero infrastructure, leverages model's training data
- **Cons**: Can hallucinate guidelines, not verifiable

**Recommendation**: Start with Option C (zero cost), upgrade to Option A (Gemini grounding) for demo impressiveness, use Option B for strongest hackathon submission.
**Effort**: ğŸŸ¡ Medium (C) to ğŸ”´ Very Large (B) | **Impact**: ğŸ”´ High (prize differentiator)

---

### 4B. Fine-Tune MedGemma for Debate ğŸ†

**From**: [STURGEON_PROJECT_PLAN.md L513](file:///c:/Users/weeki/Documents/GitHub/Sturgeon/STURGEON_PROJECT_PLAN.md#L513)
**Concept**: Fine-tune MedGemma on debate-style clinical reasoning examples.
**Approaches**:

#### LoRA Fine-Tuning (Most Practical)

- Use `peft` (Parameter-Efficient Fine-Tuning) with LoRA adapters
- Training data: Synthetic debate transcripts generated by Gemini from medical case studies
- Target: Improve JSON compliance, evidence citation quality, and differential updates
- **Hardware**: 16GB VRAM is sufficient for LoRA on 4B model
- **Deps**: `peft`, `trl`, `datasets`
- **Data generation workflow**:
  1. Scrape de-identified case studies from medical education sites
  2. Use Gemini to generate debate transcripts (challenge-response pairs)
  3. Format as instruction-tuning examples
  4. Fine-tune with LoRA (rank 8-16, alpha 16-32)

#### Prompt Tuning (Lighter)

- Instead of weight updates, learn optimal soft prompt embeddings
- Requires fewer examples (~100 vs ~1000 for LoRA)
- Less impactful but faster to iterate

**Effort**: ğŸ”´ Very Large (8-16h+) | **Impact**: ğŸ”´ High (quality, but risk of overfitting)

---

### 4C. Multi-Modal RAG (Images in Knowledge Base)

**Concept**: Store reference medical images (annotated X-rays, derm atlases) and retrieve similar cases during image analysis.
**Implementation**: Use MedSigLIP embeddings as the retrieval key â€” embed uploaded image, find nearest neighbors in a reference dataset.
**Why it's interesting**: "Here are 3 similar cases from our reference database" alongside MedGemma's analysis.
**Effort**: ğŸ”´ Very Large | **Impact**: ğŸŸ¡ Medium (impressive but complex)

---

### 4D. Gemini as Full Agentic Controller (Tool Use)

**Current**: Gemini orchestrates in a hard-coded 3-step flow (formulate â†’ MedGemma â†’ synthesize).
**Upgrade**: Use Gemini's native function calling / tool use:

- Define MedGemma as a tool (`query_medical_specialist`)
- Define RAG retrieval as a tool (`search_clinical_guidelines`)
- Define lab lookup as a tool (`lookup_lab_reference_range`)
- Let Gemini decide which tools to call and in what order
  **Architecture**: True agentic â€” Gemini autonomously decides the workflow.
  **Why it matters**: Maps directly to the "Agentic Workflow Prize" criteria.
  **Effort**: ğŸ”´ Large (4-6h) | **Impact**: ğŸ”´ High (prize alignment)

---

### 4E. Case History / Case Library

**Concept**: Save completed cases to a persistent store. Allow re-loading previous cases.
**Implementation**:

- Save to IndexedDB or a simple JSON file on the server
- Add a "Case Library" page listing past cases with diagnosis, date, round count
- Click to re-open and review or continue a case
  **Effort**: ğŸŸ¡ Medium (3-4h) | **Impact**: ğŸŸ¡ Medium

---

## 5. DevOps & Infrastructure

### 5A. Docker Compose Setup

**Current**: Manual startup of both frontend and backend.
**Fix**: Create `docker-compose.yml` with:

- `ai-service` container (Python + CUDA/ROCm)
- `frontend` container (Node.js)
- Shared network
- Volume mounts for models
  **Effort**: ğŸŸ¡ Medium (2-3h) | **Impact**: ğŸŸ¡ Medium (reproducibility)

---

### 5B. CI/CD Pipeline

**Current**: None.
**Fix**: GitHub Actions workflow:

- Lint (ESLint + ruff)
- Type check (TypeScript + mypy)
- Unit tests (pytest)
- Build check (Next.js build)
  **Effort**: ğŸŸ¡ Medium (2-3h) | **Impact**: ğŸŸ¡ Medium (code quality)

---

### 5C. Logging & Observability

**Current**: Basic `logger.info/warning/error` scattered throughout.
**Fix**:

- Structured JSON logging (for production)
- Request ID tracking across frontend â†’ API route â†’ backend
- Timing metrics for each phase (MedGemma inference, Gemini call, JSON parsing)
- Log rotation / size limits
  **Effort**: ğŸŸ¡ Medium (2-3h) | **Impact**: ğŸŸ¢ Low (debugging aid)

---

### 5D. Environment Variable Validation

**Current**: `.env` file with `GEMINI_API_KEY` and `HF_TOKEN`. No validation on startup.
**Fix**: Use `pydantic-settings` for typed environment config with defaults, validation, and helpful error messages.
**Effort**: ğŸŸ¢ Small (30 min) | **Impact**: ğŸŸ¢ Low

---

## 6. Model & Inference

### 6A. Inference Caching / Memoization

**Current**: Every identical prompt re-runs full MedGemma inference (~5-10s).
**Fix**: Add a simple LRU cache keyed on prompt hash. Useful when user refreshes page or retries same challenge.
**Effort**: ğŸŸ¢ Small (30 min) | **Impact**: ğŸŸ¢ Low (edge case optimization)

---

### 6B. Batch Inference

**Current**: Image analysis + differential run sequentially on upload page.
**Fix**: If both image and lab file are uploaded, run MedGemma image analysis and lab extraction in parallel using Python `asyncio.gather()`.
**Current state**: Frontend already does `Promise.all` for parallel API calls, but backend processes them sequentially within MedGemma (single model, single GPU).
**Effort**: ğŸŸ¡ Medium (needs careful GPU memory management) | **Impact**: ğŸŸ¢ Low

---

### 6C. Model Quantization Experimentation

**Current**: bfloat16 (~8-10GB VRAM).
**Exploration**: Try GPTQ or AWQ 4-bit quantization:

- Reduces VRAM to ~3-4GB
- Frees memory for simultaneous model loading or larger context
- May degrade quality â€” needs A/B testing
  **Effort**: ğŸŸ¡ Medium | **Impact**: ğŸŸ¢ Low (unless VRAM constrained)

---

## Priority Matrix

| #   | Improvement        | Effort | Impact  | Recommended?         |
| --- | ------------------ | ------ | ------- | -------------------- |
| 1A  | Input Validation   | ğŸŸ¢ S   | ğŸ”´ High | âœ… **Yes**           |
| 1B  | Suggested Prompts  | ğŸŸ¡ M   | ğŸ”´ High | âœ… **Yes**           |
| 1C  | Export as PDF      | ğŸŸ¢ S   | ğŸŸ¡ Med  | âœ… **Yes**           |
| 1D  | Loading States     | ğŸŸ¢ S   | ğŸŸ¡ Med  | âœ… **Yes**           |
| 2A  | Split main.py      | ğŸŸ¢ S   | ğŸŸ¡ Med  | âœ… **Yes**           |
| 2B  | Unit Tests         | ğŸŸ¡ M   | ğŸŸ¡ Med  | âœ… **Yes**           |
| 2C  | Error Handling     | ğŸŸ¡ M   | ğŸ”´ High | âœ… **Yes**           |
| 2D  | Request Validation | ğŸŸ¢ S   | ğŸŸ¡ Med  | âœ… **Yes**           |
| 2G  | CORS Config        | ğŸŸ¢ S   | ğŸŸ¡ Med  | âœ… **Yes**           |
| 3D  | Image Context      | ğŸŸ¢ S   | ğŸŸ¡ Med  | âœ… **Yes**           |
| 4A  | RAG (Option Câ†’A)   | ğŸŸ¡ M   | ğŸ”´ High | â­ **High value**    |
| 4D  | Gemini Tool Use    | ğŸ”´ L   | ğŸ”´ High | â­ **Prize-aligned** |
| 1E  | Dark Mode          | ğŸŸ¡ M   | ğŸŸ¢ Low  | ğŸ¤· Optional          |
| 1F  | Mobile Layout      | ğŸŸ¡ M   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 1G  | Accessibility      | ğŸŸ¡ M   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 1H  | Sidebar Charts     | ğŸŸ¡ M   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 2E  | Streaming (SSE)    | ğŸ”´ L   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 2F  | Session TTL        | ğŸŸ¢ S   | ğŸŸ¢ Low  | ğŸ¤· Optional          |
| 3A  | Few-Shot Prompts   | ğŸŸ¡ M   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 3B  | CoT Prompts        | ğŸŸ¢ S   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 3C  | Confidence Scale   | ğŸŸ¢ S   | ğŸŸ¢ Low  | ğŸ¤· Optional          |
| 4B  | Fine-Tune MedGemma | ğŸ”´ XL  | ğŸ”´ High | âŒ Time risk         |
| 4C  | Multi-Modal RAG    | ğŸ”´ XL  | ğŸŸ¡ Med  | âŒ Time risk         |
| 4E  | Case Library       | ğŸŸ¡ M   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 5A  | Docker Compose     | ğŸŸ¡ M   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 5B  | CI/CD              | ğŸŸ¡ M   | ğŸŸ¡ Med  | ğŸ¤· Optional          |
| 6A  | Inference Cache    | ğŸŸ¢ S   | ğŸŸ¢ Low  | ğŸ¤· Optional          |
